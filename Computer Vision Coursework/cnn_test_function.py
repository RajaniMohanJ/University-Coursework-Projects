# -*- coding: utf-8 -*-
"""CNN test_function.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bap39Ujaz0HKk5hXSM9dxxbvdA7Fzkfd

# Computer Vision Coursework Submission (IN3060/INM460)

**Student name, ID and cohort:** Rajani Mohan Janipalli (210049506) - PG

# Notebook Setup
In this section you should include all the code cells required to test your coursework submission. Specifically:

### Mount Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Define Local Path

In the next cell you should assign to the variable `GOOGLE_DRIVE_PATH_AFTER_MYDRIVE` the relative path of this folder in your Google Drive.

**IMPORTANT:** you have to make sure that **all the files required to test your functions are loaded using this variable** (as was the case for all lab tutorials). In other words, do not use in the notebook any absolute paths. This will ensure that the markers can run your functions. Also, **do not use** the magic command `%cd` to change directory.


"""

import os

# TODO: Fill in the Google Drive path where you uploaded the CW_folder_PG
# Example: GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/Computer Vision/CW_folder_PG'

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/Computer Vision Coursework/CW_Folder_PG'
GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print(os.listdir(GOOGLE_DRIVE_PATH))

"""### Load packages 

In the next cell you should load all the packages required to test your functions.

Note: Please run the below cell twice to avoid error display.
"""

!pip install opencv-python==4.5.5.64

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
from joblib import dump, load
import pickle

!pip show opencv-python
import cv2
from sklearn.model_selection import train_test_split
from skimage import img_as_ubyte, io, color
from sklearn.cluster import MiniBatchKMeans
from sklearn import svm, metrics
from collections import Counter

# %matplotlib inline

import pandas as pd
from torch.utils.data import Dataset
import pandas as pd
from PIL import Image
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from tqdm import tqdm
import torchvision
import torchvision.transforms as transforms
import torch.nn.functional as F
import torch.optim as optim
from torchvision import models
from torch.optim import lr_scheduler
from google.colab.patches import cv2_imshow
import pickle

# Identify path to zipped dataset
CW_zip_path = os.path.join(GOOGLE_DRIVE_PATH, 'CW_Dataset', 'CW_Dataset.zip')

# Copy it to Colab
!cp '{CW_zip_path}' .

# Unzip it
!yes|unzip -q CW_Dataset.zip

# Delete zipped version from Colab (not from Drive)
!rm CW_Dataset.zip

device = ("cuda" if torch.cuda.is_available() else "cpu")

"""### Load models

In the next cell you should load all your trained models for easier testing of your functions. Avoid to load them within `EmotionRecognition` and `EmotionRecognitionVideo` to avoid having to reload them each time.
"""

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 22 * 22, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 8)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(x.size(0), 16 * 22 * 22)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

CNNMODELPATH = os.path.join(GOOGLE_DRIVE_PATH, 'Models/CNN_FER.pt')
net = Net()
net.load_state_dict(torch.load(CNNMODELPATH))
net.eval()

models.vgg16(pretrained = True)

VGG16CNN1MODELPATH = os.path.join(GOOGLE_DRIVE_PATH, 'Models/VGG16CNN1_FER.pt')

device = torch.device('cpu')
vgg16cnn = models.vgg16(pretrained = True)
vgg16cnn.load_state_dict(torch.load(VGG16CNN1MODELPATH, map_location=device))

"""# Test EmotionRecognition

This section should allow a quick test of the `EmotionRecognition` function. First, add cells with the code needed to load the necessary subroutines to make `EmotionRecognition` work.
"""

path_to_testset = os.path.join(GOOGLE_DRIVE_PATH, 'CW_Dataset')

def EmotionRecognition(path_to_testset, model_name):
  
  def create_imagename_dataframe(path):
      """Create a dataframe of images and labels from selected directories"""
      data_df = pd.DataFrame(columns=["imgname", "label"])
      data_df["imgname"] = [file for file in sorted(os.listdir(os.path.join(path))) if file.endswith('.jpg')]
      label_set = np.loadtxt(os.path.join('labels', 'list_label_{}.txt'.format(path)), dtype='str')
      label_nums = [] # create an empty list to append label numbers.
      for i in range(len(label_set)): # execute a for loop to extract the exact labels from data and append them to a list.
        label_nums.append(label_set[i][1])
      
      data_df["label"] = label_nums

      data_df.to_csv (r'{}_df_csv.csv'.format(path), index = False, header=True)
      
      print('Create datafame with file name {}_df_csv.csv'.format(path))
    
  create_imagename_dataframe('test')

  def imagedisplay(img):
    img = img / 2 + 0.5     # Unnormalize: back to range [0, 1] just for showing the images
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))     # Reshape: C, H, W -> H, W, C
    plt.show()

  class CreateDataset(Dataset):
    def __init__(self, root_dir, annotation_file, transform=None):
        self.root_dir = root_dir
        self.annotations = pd.read_csv(annotation_file)
        self.transform = transform

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
          
        img_id = self.annotations.iloc[idx, 0]
        img = Image.open(os.path.join(self.root_dir, img_id))
        y_label = self.annotations.iloc[idx, 1]

        if self.transform is not None:
            img = self.transform(img)

        return (img, y_label)
    
  transform = transforms.Compose(
          [
              transforms.ToTensor(),
              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
          ]
      )
  test_dataset = CreateDataset("test","test_df_csv.csv",transform=transform)
  test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=16,num_workers=1)

  if model_name == 'CNN':

    correct = 0
    total = 0
    with torch.no_grad():             # Avoid backprop at test 
        for data in test_loader:
            images, labels = data
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f"Accuracy of the CNN on the 3068 test images: {100 * correct / total}%")
    imagedisplay(torchvision.utils.make_grid(images))
    print('Ground-truth:', labels)
    print('Ground-truth:', predicted)
    print('Label explanation: 1-Surprise, 2-Fear, 3-Disgust, 4-Happiness, 5-Sadness, 6-Anger, 7-Neutral')
  
  elif model_name == 'VGG16CNN1':
    
    correct = 0
    total = 0
    with torch.no_grad():             # Avoid backprop at test 
      for data in test_loader:
        vgg16cnn.cuda()
        images, labels = data
        outputs = vgg16cnn(images.cuda())
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels.cuda()).sum().item()

    print(f"Accuracy of the network on the 3068 test images: {100 * correct / total}%")
    imagedisplay(torchvision.utils.make_grid(images))
    print('Ground-truth:', labels)
    print('Ground-truth:', predicted)
    print('Label explanation: 1-Surprise, 2-Fear, 3-Disgust, 4-Happiness, 5-Sadness, 6-Anger, 7-Neutral')

"""Then, make a call to the `EmotionRecognition` function to see what results it produces. You must also indicate the syntax needed to test your different models."""

EmotionRecognition(path_to_testset, 'CNN')

EmotionRecognition(path_to_testset, 'VGG16CNN1')

"""# Test EmotionRecognitionVideo

This section should allow a quick test of the `EmotionRecognitionVideo` function. First, add cells with the code needed to load the necessary subroutines to make `EmotionRecognitionVideo` work.
"""

video_input3 = os.path.join(GOOGLE_DRIVE_PATH, 'Video/EmotionalInterviewRDJr3.mp4')

def EmotionRecognitionVideo1(model, video_input):

  video = cv2.VideoCapture(video_input)

  transform = transforms.Compose(
            [
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
            ]
        )

  # if (video.isOpened() == False):
  #     print("An error occured while trying to read video")
  # frame_width = int(video.get(3))
  # frame_height = int(video.get(4))

  while(video.isOpened()):
      ret, frame = video.read()
      if ret == True:
        model.eval()
        with torch.no_grad():
          
          haar_cas = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
          faces_detected = haar_cas.detectMultiScale(frame, scaleFactor = 1.2, minNeighbors=5)
          
          for (x, y, w, h) in faces_detected:
            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0),2)

            converted_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            trans_frame = transform(converted_image)
            trans_frame.cuda()
            model.cuda()

            outputs = model(trans_frame.unsqueeze(0).cuda())
            _, preds = torch.max(outputs.data, 1)

            labels = {1: "Surprise", 2: "Fear", 3: "Disgust", 4: "Happiness", 5: "Sadness", 6: "Anger", 7: "Neutral"}
            cv2.putText(frame, labels[preds.item()], (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0),2)
            cv2_imshow(frame)
        # out.write(frame)
        if cv2.waitKey(1) & 0xFF == ord("q"):
          break
      else:
        break

  video.release()
  cv2.destroyAllWindows()


  ## CODING REFERENCES:
  ## https://github.com/Conero007/Emotion-Detection-Pytorch/blob/master/video_capture.py
  ## https://github.com/vjgpt/Face-and-Emotion-Recognition/blob/master/face-rec-emotion.py

"""Then, make a call to the `EmotionRecognitionVideo` function to see what results it produces."""

EmotionRecognitionVideo1(vgg16cnn, video_input3)